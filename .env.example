# Database Configuration
# Use postgresql+asyncpg:// for async SQLAlchemy runtime
# Alembic will automatically convert to postgresql+psycopg2:// for migrations
DATABASE_URL=postgresql+asyncpg://admin:admin@localhost:5432/ragitect_db

# Encryption Configuration
# Generate key with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
ENCRYPTION_KEY=your-encryption-key-here

# LLM Configuration
LLM_PROVIDER="your-llm-provider"
LLM_MODEL="your-llm-model"
LLM_API_KEY="your-llm-api-key"
LLM_TEMPERATURE=0.7

# Document Processing Configuration (Story 3.3.A - Token-Based Chunking)
# CHUNK_SIZE_TOKENS: Target chunk size in tokens (default: 512)
# CHUNK_OVERLAP_TOKENS: Overlap between chunks in tokens (default: 50)
# MIN_CHUNK_SIZE_TOKENS: Minimum chunk size to prevent orphan headers (default: 64)
# Token-based sizing provides consistent results across embedding models
CHUNK_SIZE_TOKENS=512
CHUNK_OVERLAP_TOKENS=50
MIN_CHUNK_SIZE_TOKENS=64

# Embedding Configuration (Story 3.3.A - Batch Processing)
# EMBEDDING_PROVIDER: Provider name (ollama, openai, vertex_ai)
# EMBEDDING_MODEL: Model identifier (default: qwen3-embedding:0.6b)
# EMBEDDING_BATCH_SIZE: Max embeddings per API call (default: 32)
# Batching prevents API limit errors with large documents
EMBEDDING_PROVIDER=ollama
EMBEDDING_MODEL=qwen3-embedding:0.6b
EMBEDDING_BASE_URL=http://localhost:11434
EMBEDDING_DIMENSION=768
EMBEDDING_BATCH_SIZE=32

# Multi-Stage Retrieval Pipeline Configuration (Story 3.1.2)
# RETRIEVAL_INITIAL_K: Number of candidates to over-retrieve (default: 50)
# RETRIEVAL_RERANKER_TOP_K: Number of top reranked chunks to pass to MMR (default: 30)
# RETRIEVAL_MMR_K: Number of diverse chunks to select via MMR (default: 20)
# RETRIEVAL_USE_RERANKER: Enable cross-encoder reranking (default: True)
# RETRIEVAL_USE_MMR: Enable MMR diversity selection (default: True)
# RETRIEVAL_USE_ADAPTIVE_K: Enable adaptive K selection based on score gaps (default: True)
# RETRIEVAL_MMR_LAMBDA: Balance between relevance (1.0) and diversity (0.0) (default: 0.7)
# RETRIEVAL_ADAPTIVE_K_MIN: Minimum chunks to return (default: 4)
# RETRIEVAL_ADAPTIVE_K_MAX: Maximum chunks to return (default: 16)
# RETRIEVAL_ADAPTIVE_K_GAP_THRESHOLD: Minimum score gap to consider significant (default: 0.15)
#   - Lower values (0.10): More aggressive cutting, returns fewer chunks
#   - Higher values (0.25): More conservative, returns more chunks
#   - Range: 0.05-0.30 (typical rerank scores are 0.0-1.0)
# RETRIEVAL_TOKEN_BUDGET: Max tokens for context (default: 4000, optional)
RETRIEVAL_INITIAL_K=50
RETRIEVAL_RERANKER_TOP_K=30
RETRIEVAL_MMR_K=20
RETRIEVAL_USE_RERANKER=True
RETRIEVAL_USE_MMR=True
RETRIEVAL_USE_ADAPTIVE_K=True
RETRIEVAL_MMR_LAMBDA=0.7
RETRIEVAL_ADAPTIVE_K_MIN=4
RETRIEVAL_ADAPTIVE_K_MAX=16
RETRIEVAL_ADAPTIVE_K_GAP_THRESHOLD=0.15
RETRIEVAL_TOKEN_BUDGET=4000
