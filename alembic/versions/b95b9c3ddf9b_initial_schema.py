"""initial_schema

Revision ID: b95b9c3ddf9b
Revises:
Create Date: 2025-11-24 11:17:00.889443

"""

from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
from pgvector.sqlalchemy import Vector

# revision identifiers, used by Alembic.
revision: str = "b95b9c3ddf9b"
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # Enable pgvector extension
    op.execute("CREATE EXTENSION IF NOT EXISTS vector")

    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "workspaces",
        sa.Column(
            "id", sa.UUID(), nullable=False, comment="Unique workspace identifier"
        ),
        sa.Column(
            "name",
            sa.String(length=255),
            nullable=False,
            comment="Workspace name (unique)",
        ),
        sa.Column(
            "description",
            sa.Text(),
            nullable=True,
            comment="Optional workspace description",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
            comment="Creation timestamp",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
            comment="Last update timestamp",
        ),
        sa.CheckConstraint("length(name) > 0", name="workspace_name_not_empty"),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("name"),
        comment="Workspaces for organizing document collections",
    )
    op.create_table(
        "documents",
        sa.Column(
            "id", sa.UUID(), nullable=False, comment="Unique document identifier"
        ),
        sa.Column(
            "workspace_id", sa.UUID(), nullable=False, comment="Parent workspace id"
        ),
        sa.Column(
            "file_name",
            sa.String(length=512),
            nullable=False,
            comment="Original file name",
        ),
        sa.Column(
            "file_type",
            sa.String(length=50),
            nullable=True,
            comment="File type or extension (e.g., 'pdf', 'docx')",
        ),
        sa.Column(
            "content_hash",
            sa.String(length=64),
            nullable=False,
            comment="SHA-256 hash of 'workspace_id:content'",
        ),
        sa.Column(
            "unique_identifier_hash",
            sa.String(length=64),
            nullable=False,
            comment="Unique hash accross the entire system",
        ),
        sa.Column(
            "processed_content",
            sa.Text(),
            nullable=True,
            comment="Full text content after ETL processing (should be md)",
        ),
        sa.Column(
            "summary", sa.Text(), nullable=True, comment="Optional document summary"
        ),
        sa.Column(
            "embedding",
            Vector(768),
            nullable=True,
            comment="Document-level embedding vector (768 dims)",
        ),
        sa.Column(
            "processed_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
            comment="Processing completion timestamp",
        ),
        sa.Column(
            "metadata",
            postgresql.JSONB(astext_type=sa.Text()),
            nullable=True,
            comment="Additional metadata (embedding model, page count, etc.)",
        ),
        sa.ForeignKeyConstraint(
            ["workspace_id"], ["workspaces.id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("unique_identifier_hash"),
        sa.UniqueConstraint(
            "workspace_id", "content_hash", name="uq_workspace_content_hash"
        ),
        comment="Processed documents with embeddings",
    )
    op.create_index(
        op.f("ix_documents_content_hash"), "documents", ["content_hash"], unique=False
    )
    op.create_index(
        op.f("ix_documents_workspace_id"), "documents", ["workspace_id"], unique=False
    )
    op.create_table(
        "document_chunks",
        sa.Column(
            "id", sa.UUID(), nullable=False, comment="Unique document chunk identifier"
        ),
        sa.Column(
            "document_id", sa.UUID(), nullable=False, comment="Parent document id"
        ),
        sa.Column(
            "workspace_id", sa.UUID(), nullable=False, comment="Parent workspace id"
        ),
        sa.Column(
            "chunk_index",
            sa.Integer(),
            nullable=False,
            comment="Index of the chunk within the document (0-based)",
        ),
        sa.Column("content", sa.Text(), nullable=False, comment="Chunk text content"),
        sa.Column(
            "embedding",
            Vector(768),
            nullable=False,
            comment="Chunk-level embedding vector (768 dims)",
        ),
        sa.Column(
            "metadata",
            postgresql.JSONB(astext_type=sa.Text()),
            nullable=True,
            comment="Additional metadata (page_number, section, etc.)",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
            comment="Creation timestamp",
        ),
        sa.CheckConstraint("chunk_index >= 0", name="chunk_index_non_negative"),
        sa.CheckConstraint("length(content) > 0", name="chunk_content_not_empty"),
        sa.ForeignKeyConstraint(["document_id"], ["documents.id"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(
            ["workspace_id"], ["workspaces.id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint(
            "document_id", "chunk_index", name="uq_document_chunk_index"
        ),
        comment="Document chunks with embeddings for retrieval",
    )
    op.create_index(
        op.f("ix_document_chunks_document_id"),
        "document_chunks",
        ["document_id"],
        unique=False,
    )
    op.create_index(
        op.f("ix_document_chunks_workspace_id"),
        "document_chunks",
        ["workspace_id"],
        unique=False,
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f("ix_document_chunks_workspace_id"), table_name="document_chunks")
    op.drop_index(op.f("ix_document_chunks_document_id"), table_name="document_chunks")
    op.drop_table("document_chunks")
    op.drop_index(op.f("ix_documents_workspace_id"), table_name="documents")
    op.drop_index(op.f("ix_documents_content_hash"), table_name="documents")
    op.drop_table("documents")
    op.drop_table("workspaces")
    # ### end Alembic commands ###

    # Drop pgvector extension
    op.execute("DROP EXTENSION IF EXISTS vector")
